import logging
import os
import asyncio
import time
from datetime import datetime
from typing import Optional, Any
import uuid
from collections import deque
import re
import csv
import json

from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI, OpenAIError, APITimeoutError
import numpy as np
import pickle

# ë‰´ìŠ¤ í¬ë¡¤ë§ìš©
import requests
from bs4 import BeautifulSoup

# ğŸ†• ë‰´ìŠ¤ í•„í„°ë§ ì‹œìŠ¤í…œ
try:
    from news_filter_simple import filter_real_estate_news, filter_news_batch
    NEWS_FILTER_AVAILABLE = True
except ImportError:
    NEWS_FILTER_AVAILABLE = False
    logging.warning("âš ï¸ news_filter_simple.py not found - filtering disabled")

# Google Sheetsìš©
try:
    import gspread
    from google.oauth2.service_account import Credentials
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False
    logging.warning("gspread not installed. Google Sheets logging disabled.")

# Redis for queue management
try:
    import redis.asyncio as redis
    from redis.asyncio import Redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    Redis = Any
    logging.warning("redis package not installed. Using in-memory queue.")

# ================================================================================
# Logging Configuration
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="REXA - Real Estate Expert Assistant",
    description="Solar API + RAG chatbot for real estate + News QA",
    version="2.0.0"
)

# ================================================================================
# Configuration & Global Variables
# ================================================================================

# Naver News API
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# Google Sheets Configuration
GOOGLE_SHEETS_CREDENTIALS = os.getenv("GOOGLE_SHEETS_CREDENTIALS")
GOOGLE_SHEETS_SPREADSHEET_ID = os.getenv("GOOGLE_SHEETS_SPREADSHEET_ID")

# CSV íŒŒì¼ ê²½ë¡œ
CSV_FILE_PATH = "news_log.csv"

# Google Sheets í´ë¼ì´ì–¸íŠ¸ (ì „ì—­)
gsheet_client = None
gsheet_worksheet = None

# Redis Configuration
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

# Health Check Configuration
HEALTH_CHECK_INTERVAL = int(os.getenv("HEALTH_CHECK_INTERVAL", 5))
MAX_UNHEALTHY_COUNT = int(os.getenv("MAX_UNHEALTHY_COUNT", 3))

# Queue Configuration
WEBHOOK_QUEUE_NAME = "rexa:webhook_queue"
WEBHOOK_PROCESSING_QUEUE = "rexa:processing_queue"
WEBHOOK_FAILED_QUEUE = "rexa:failed_queue"
MAX_RETRY_ATTEMPTS = int(os.getenv("MAX_RETRY_ATTEMPTS", 3))
QUEUE_PROCESS_INTERVAL = int(os.getenv("QUEUE_PROCESS_INTERVAL", 5))

# API Timeout Configuration
API_TIMEOUT = int(os.getenv("API_TIMEOUT", 3))

# Global state
redis_client: Optional[Any] = None
server_healthy = True
unhealthy_count = 0
last_health_check = datetime.now()

# In-memory queue fallback
in_memory_webhook_queue: deque = deque()
in_memory_processing_queue: deque = deque()
in_memory_failed_queue: deque = deque()
use_in_memory_queue = False

# News session storage (user_id -> news_data)
news_sessions = {}

# ================================================================================
# Google Sheets & CSV Initialization
# ================================================================================

def init_google_sheets():
    """Initialize Google Sheets client"""
    global gsheet_client, gsheet_worksheet
    
    if not GSPREAD_AVAILABLE:
        logger.error("âŒ gspread not installed - Google Sheets logging disabled")
        logger.error("   Install: pip install gspread google-auth --break-system-packages")
        return False
    
    if not GOOGLE_SHEETS_CREDENTIALS:
        logger.error("âŒ GOOGLE_SHEETS_CREDENTIALS environment variable not set")
        logger.error("   Set in Render: Environment â†’ GOOGLE_SHEETS_CREDENTIALS")
        return False
    
    if not GOOGLE_SHEETS_SPREADSHEET_ID:
        logger.error("âŒ GOOGLE_SHEETS_SPREADSHEET_ID environment variable not set")
        logger.error("   Set in Render: Environment â†’ GOOGLE_SHEETS_SPREADSHEET_ID")
        return False
    
    try:
        logger.info("ğŸ”„ Initializing Google Sheets...")
        
        # JSON ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ íŒŒì‹±
        try:
            creds_dict = json.loads(GOOGLE_SHEETS_CREDENTIALS)
            logger.info("âœ… Credentials JSON parsed successfully")
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Invalid JSON in GOOGLE_SHEETS_CREDENTIALS: {e}")
            return False
        
        # Credentials ìƒì„±
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]
        credentials = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        logger.info("âœ… Google credentials created")
        
        # gspread í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        gsheet_client = gspread.authorize(credentials)
        logger.info("âœ… gspread client authorized")
        
        # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸°
        spreadsheet = gsheet_client.open_by_key(GOOGLE_SHEETS_SPREADSHEET_ID)
        gsheet_worksheet = spreadsheet.sheet1
        logger.info(f"âœ… Spreadsheet opened: {spreadsheet.title}")
        
        # í—¤ë” í™•ì¸ ë° ìƒì„±
        try:
            headers = gsheet_worksheet.row_values(1)
            if not headers or headers[0] != 'timestamp':
                # ğŸ†• ìƒˆë¡œìš´ ì»¬ëŸ¼ êµ¬ì¡°
                gsheet_worksheet.insert_row([
                    'timestamp', 'title', 'description', 'url',
                    'is_relevant', 'relevance_score', 'keywords', 'region',
                    'has_price', 'has_policy', 'reason', 'user_id'
                ], 1)
                logger.info("âœ… Google Sheets headers created (with filtering columns)")
            else:
                logger.info(f"âœ… Google Sheets headers found: {headers}")
        except Exception as e:
            # ğŸ†• ìƒˆë¡œìš´ ì»¬ëŸ¼ êµ¬ì¡°
            gsheet_worksheet.insert_row([
                'timestamp', 'title', 'description', 'url',
                'is_relevant', 'relevance_score', 'keywords', 'region',
                'has_price', 'has_policy', 'reason', 'user_id'
            ], 1)
            logger.info("âœ… Google Sheets headers created (with filtering columns)")
        
        logger.info(f"âœ… Google Sheets initialized: {GOOGLE_SHEETS_SPREADSHEET_ID}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Google Sheets: {type(e).__name__}: {e}")
        import traceback
        logger.error(f"   Traceback: {traceback.format_exc()}")
        return False

def init_csv_file():
    """Initialize CSV file with headers"""
    try:
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
        if not os.path.exists(CSV_FILE_PATH):
            with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                # ğŸ†• ìƒˆë¡œìš´ ì»¬ëŸ¼ êµ¬ì¡°
                writer.writerow([
                    'timestamp', 'title', 'description', 'url',
                    'is_relevant', 'relevance_score', 'keywords', 'region',
                    'has_price', 'has_policy', 'reason', 'user_id'
                ])
            logger.info(f"âœ… CSV file created: {CSV_FILE_PATH}")
        else:
            logger.info(f"âœ… CSV file exists: {CSV_FILE_PATH}")
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to initialize CSV: {e}")
        return False

def save_news_to_csv(news_data: dict):
    """Save news to CSV file with filtering metadata"""
    try:
        with open(CSV_FILE_PATH, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # ğŸ†• ìƒˆë¡œìš´ ì»¬ëŸ¼ êµ¬ì¡°
            writer.writerow([
                news_data['timestamp'],
                news_data['title'],
                news_data['description'],
                news_data['url'],
                news_data.get('is_relevant', True),
                news_data.get('relevance_score', 0),
                ', '.join(news_data.get('keywords', [])),
                news_data.get('region', ''),
                news_data.get('has_price', False),
                news_data.get('has_policy', False),
                news_data.get('reason', ''),
                news_data['user_id']
            ])
        logger.info(f"âœ… News saved to CSV: {news_data['title'][:30]}...")
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to save to CSV: {e}")
        return False

def save_news_to_gsheet(news_data: dict):
    """Save news to Google Sheets with filtering metadata"""
    if not gsheet_worksheet:
        logger.warning("âš ï¸ Google Sheets not initialized - skipping")
        return False
    
    try:
        # ğŸ†• ìƒˆë¡œìš´ ì»¬ëŸ¼ êµ¬ì¡°
        gsheet_worksheet.append_row([
            news_data['timestamp'],
            news_data['title'],
            news_data['description'],
            news_data['url'],
            news_data.get('is_relevant', True),
            news_data.get('relevance_score', 0),
            ', '.join(news_data.get('keywords', [])),
            news_data.get('region', ''),
            news_data.get('has_price', False),
            news_data.get('has_policy', False),
            news_data.get('reason', ''),
            news_data['user_id']
        ])
        logger.info(f"âœ… News saved to Google Sheets: {news_data['title'][:30]}...")
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to save to Google Sheets: {e}")
        return False

def save_news_log(title: str, description: str, url: str, content: str = "", user_id: str = "unknown"):
    """Save news to both CSV and Google Sheets"""
    news_data = {
        'timestamp': datetime.now().isoformat(),
        'title': title,
        'description': description,
        'url': url,
        'content': content,
        'user_id': user_id
    }
    
    # CSV ì €ì¥ (ë°±ì—…)
    save_news_to_csv(news_data)
    
    # Google Sheets ì €ì¥ (ë©”ì¸)
    save_news_to_gsheet(news_data)


# ================================================================================
# Upstage Solar API Configuration
# ================================================================================

client = OpenAI(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1/solar",
    timeout=API_TIMEOUT
)

logger.info("âœ… Upstage Solar API client configured")

# ================================================================================
# RAG - Load Embeddings
# ================================================================================

article_chunks = []
chunk_embeddings = []

try:
    with open("embeddings.pkl", "rb") as f:
        data = pickle.load(f)
        article_chunks = data["chunks"]
        chunk_embeddings = data["embeddings"]
    logger.info(f"âœ… Loaded {len(article_chunks)} chunks from embeddings.pkl")
    logger.info(f"âœ… RAG is ENABLED with {len(article_chunks)} chunks")
except FileNotFoundError:
    logger.warning("âš ï¸ embeddings.pkl not found - RAG will not be available")
    logger.warning("âš ï¸ Server will continue WITHOUT RAG - responses will be general")
    logger.warning("âš ï¸ To enable RAG: run 'python embedding2_solar.py' and redeploy")
except Exception as e:
    logger.error(f"âŒ Failed to load embeddings: {e}")
    logger.warning("âš ï¸ Server will continue WITHOUT RAG")

# ================================================================================
# News Functions
# ================================================================================

def search_naver_news(query: str = "ë¶€ë™ì‚°", display: int = 10) -> Optional[list]:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰ + ë¶€ë™ì‚° ê´€ë ¨ì„± í•„í„°ë§"""
    url = "https://openapi.naver.com/v1/search/news.json"
    
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    params = {
        "query": query,
        "display": display,
        "sort": "date"  # ìµœì‹ ìˆœ
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()
        
        items = data.get('items', [])
        if not items:
            return None
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ë§Œ í•„í„°ë§
        naver_items = [item for item in items if 'news.naver.com' in item['link']]
        
        if not naver_items:
            logger.warning("âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            naver_items = items  # í´ë°±: ëª¨ë“  ë‰´ìŠ¤ ì‚¬ìš©
        
        logger.info(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ {len(naver_items)}ê°œ ë°œê²¬")
        
        # ëª¨ë“  ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬
        processed_items = []
        for item in naver_items:
            # HTML íƒœê·¸ ì œê±°
            title = re.sub('<[^<]+?>', '', item['title'])
            description = re.sub('<[^<]+?>', '', item['description'])
            
            # HTML ì—”í‹°í‹° ë””ì½”ë”©
            import html
            title = html.unescape(title)
            description = html.unescape(description)
            
            # ìš”ì•½ ê¸¸ì´ ì œí•œ (200ì, ë¬¸ì¥ ë‹¨ìœ„ë¡œ)
            if len(description) > 200:
                cut_pos = 200
                for i in range(200, max(0, len(description) - 100), -1):
                    if description[i] in '.!?':
                        cut_pos = i + 1
                        break
                description = description[:cut_pos].strip()
            
            processed_items.append({
                "title": title,
                "description": description,
                "link": item['link'],
                "pubDate": item['pubDate'],
                "timestamp": datetime.now().isoformat()
            })
        
        # ğŸ†• ë¶€ë™ì‚° ê´€ë ¨ì„± í•„í„°ë§
        if NEWS_FILTER_AVAILABLE:
            logger.info(f"ğŸ” í•„í„°ë§ ì‹œì‘: {len(processed_items)}ê°œ ê¸°ì‚¬")
            filtered_items = filter_news_batch(processed_items)
            logger.info(
                f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(processed_items)}ê°œ ì¤‘ {len(filtered_items)}ê°œ ê´€ë ¨ ê¸°ì‚¬ "
                f"({len(filtered_items)/len(processed_items)*100:.1f}%)"
            )
            return filtered_items
        else:
            logger.warning("âš ï¸ í•„í„°ë§ ëª¨ë“ˆ ì—†ìŒ - ëª¨ë“  ê¸°ì‚¬ ë°˜í™˜")
            return processed_items
        
    except Exception as e:
        logger.error(f"âŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None

def crawl_news_content(url: str) -> str:
    """ë‰´ìŠ¤ URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ - ì „ì²´ ì›ë¬¸ (ì¬ì‹œë„ í¬í•¨)"""
    max_retries = 2
    
    for attempt in range(max_retries):
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://news.naver.com/',
                'Connection': 'keep-alive'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            if 'news.naver.com' in url:
                article = soup.select_one('#dic_area') or soup.select_one('#articeBody') or soup.select_one('.news_end')
                if article:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for tag in article.find_all(['script', 'style', 'aside']):
                        tag.decompose()
                    content = article.get_text(strip=True, separator='\n')
                    logger.info(f"ğŸ“„ í¬ë¡¤ë§ ì„±ê³µ: {len(content)}ì")
                    return content  # ì „ì²´ ì›ë¬¸ ë°˜í™˜ (ì œí•œ ì—†ìŒ)
            
            # ì¼ë°˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ - p íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ
            paragraphs = soup.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 50])
            
            if content:
                logger.info(f"ğŸ“„ í¬ë¡¤ë§ ì„±ê³µ: {len(content)}ì")
                return content  # ì „ì²´ ì›ë¬¸ ë°˜í™˜
            else:
                return "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                logger.warning(f"âš ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ - ì¬ì‹œë„ {attempt + 1}/{max_retries}")
                time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue
            else:
                logger.error(f"âŒ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {url[:50]}...")
                return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (íƒ€ì„ì•„ì›ƒ)"
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                # Too Many Requests
                if attempt < max_retries - 1:
                    wait_time = 3  # 3ì´ˆ ëŒ€ê¸°
                    logger.warning(f"âš ï¸ Rate Limit (429) - {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„")
                    time.sleep(wait_time)
                    continue
                else:
                    logger.error(f"âŒ Rate Limit ì´ˆê³¼: {url[:50]}...")
                    return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Rate Limit)"
            else:
                logger.error(f"âŒ HTTP ì˜¤ë¥˜ {e.response.status_code}: {url[:50]}...")
                return f"ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (HTTP {e.response.status_code})"
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
# ================================================================================
# RAG Helper Functions
# ================================================================================

def cosine_similarity(a, b):
    """Calculate cosine similarity between two vectors"""
    from numpy import dot
    from numpy.linalg import norm
    return dot(a, b) / (norm(a) * norm(b))

async def get_relevant_context(prompt: str, top_n: int = 2) -> str:
    """Get relevant context from embeddings for RAG"""
    if not chunk_embeddings or not article_chunks:
        logger.warning("âš ï¸ No embeddings available for RAG")
        return ""
    
    try:
        # ì„ë² ë”© ì°¨ì› ìë™ ê°ì§€
        embedding_dim = len(chunk_embeddings[0])
        logger.info(f"ğŸ“Š Detected embedding dimension: {embedding_dim}")
        
        # ì°¨ì›ì— ë”°ë¼ ì ì ˆí•œ API ì‚¬ìš©
        if embedding_dim == 1536:
            # OpenAI ì„ë² ë”© (text-embedding-3-small)
            logger.info("ğŸ”§ Using OpenAI embedding model")
            try:
                openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                q_embedding = openai_client.embeddings.create(
                    input=prompt, 
                    model="text-embedding-3-small"
                ).data[0].embedding
            except Exception as e:
                logger.error(f"âŒ OpenAI embedding failed: {e}")
                logger.info("ğŸ’¡ Set OPENAI_API_KEY environment variable")
                return ""
                
        else:
            # Solar ì„ë² ë”© (ëª¨ë“  ë‹¤ë¥¸ ì°¨ì›)
            logger.info(f"ğŸ”§ Using Solar embedding model (dimension: {embedding_dim})")
            try:
                q_embedding = client.embeddings.create(
                    input=prompt, 
                    model="solar-embedding-1-large-query"  # Solar ì¿¼ë¦¬ìš© ëª¨ë¸
                ).data[0].embedding
            except Exception as e:
                logger.error(f"âŒ Solar embedding failed: {e}")
                logger.error(f"   Model: solar-embedding-1-large-query")
                return ""
        
        # Calculate similarities
        similarities = [cosine_similarity(q_embedding, emb) for emb in chunk_embeddings]
        
        # Get top N most similar chunks
        top_indices = np.argsort(similarities)[-top_n:][::-1]
        selected_context = "\n\n".join([article_chunks[i] for i in top_indices])
        
        # Format similarities for logging
        similarity_scores = [f"{similarities[i]:.3f}" for i in top_indices]
        logger.info(f"âœ… Retrieved {top_n} relevant chunks (similarities: {similarity_scores})")
        return selected_context
        
    except Exception as e:
        logger.error(f"âŒ Error getting relevant context: {e}")
        return ""

# ================================================================================
# Pydantic Models
# ================================================================================

class DetailParams(BaseModel):
    prompt: dict

class Action(BaseModel):
    params: dict
    detailParams: dict

class RequestBody(BaseModel):
    action: Action

class QueuedRequest(BaseModel):
    request_id: str
    request_body: dict
    timestamp: str
    retry_count: int = 0
    error_message: Optional[str] = None

class HealthStatus(BaseModel):
    status: str
    model: str
    mode: str
    server_healthy: bool
    last_check: str
    redis_connected: bool
    queue_size: int
    processing_queue_size: int
    failed_queue_size: int

# ================================================================================
# Redis & Queue Management
# ================================================================================

async def init_redis():
    """Initialize Redis connection"""
    global redis_client, use_in_memory_queue
    
    if not REDIS_AVAILABLE:
        logger.warning("âš ï¸ Redis package not installed - using in-memory queue")
        use_in_memory_queue = True
        return
    
    try:
        redis_client = await redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            password=REDIS_PASSWORD,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_keepalive=True,
            retry_on_timeout=True
        )
        await redis_client.ping()
        logger.info(f"âœ… Redis connected: {REDIS_HOST}:{REDIS_PORT}")
        use_in_memory_queue = False
    except Exception as e:
        logger.warning(f"âš ï¸ Redis connection failed: {e}")
        logger.warning("âš ï¸ Using in-memory queue as fallback")
        use_in_memory_queue = True

async def close_redis():
    """Close Redis connection"""
    global redis_client
    if redis_client:
        await redis_client.close()
        logger.info("âœ… Redis connection closed")

async def enqueue_webhook_request(request_id: str, request_body: dict):
    """Add webhook request to queue"""
    queued_req = QueuedRequest(
        request_id=request_id,
        request_body=request_body,
        timestamp=datetime.now().isoformat()
    )
    
    if use_in_memory_queue:
        in_memory_webhook_queue.append(queued_req)
        logger.info(f"ğŸ“¥ Request {request_id[:8]} added to in-memory queue (size: {len(in_memory_webhook_queue)})")
        return
    
    if not redis_client:
        in_memory_webhook_queue.append(queued_req)
        logger.warning(f"âš ï¸ Redis unavailable - using in-memory queue")
        return
    
    try:
        await redis_client.lpush(WEBHOOK_QUEUE_NAME, queued_req.model_dump_json())
        queue_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        logger.info(f"ğŸ“¥ Request {request_id[:8]} added to Redis queue (size: {queue_size})")
    except Exception as e:
        logger.error(f"âŒ Failed to enqueue to Redis: {e}")
        in_memory_webhook_queue.append(queued_req)
        logger.info(f"ğŸ“¥ Fallback to in-memory queue (size: {len(in_memory_webhook_queue)})")

async def get_queue_sizes():
    """Get sizes of all queues"""
    if use_in_memory_queue:
        return (
            len(in_memory_webhook_queue),
            len(in_memory_processing_queue),
            len(in_memory_failed_queue)
        )
    
    if not redis_client:
        return (0, 0, 0)
    
    try:
        webhook_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        processing_size = await redis_client.llen(WEBHOOK_PROCESSING_QUEUE)
        failed_size = await redis_client.llen(WEBHOOK_FAILED_QUEUE)
        return (webhook_size, processing_size, failed_size)
    except Exception as e:
        logger.error(f"âŒ Failed to get queue sizes: {e}")
        return (0, 0, 0)

async def health_check_monitor():
    """Background task to monitor server health"""
    global server_healthy, unhealthy_count, last_health_check
    
    while True:
        try:
            await asyncio.sleep(HEALTH_CHECK_INTERVAL)
            
            # Check Solar API
            try:
                test_response = client.chat.completions.create(
                    model="solar-mini",
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=10,
                    timeout=2
                )
                
                server_healthy = True
                unhealthy_count = 0
                last_health_check = datetime.now()
                logger.debug(f"âœ… Health check passed at {last_health_check}")
                
            except Exception as e:
                unhealthy_count += 1
                logger.warning(f"âš ï¸ Health check failed ({unhealthy_count}/{MAX_UNHEALTHY_COUNT}): {e}")
                
                if unhealthy_count >= MAX_UNHEALTHY_COUNT:
                    server_healthy = False
                    logger.error(f"âŒ Server marked as unhealthy after {unhealthy_count} failures")
                
        except Exception as e:
            logger.error(f"âŒ Health check monitor error: {e}")

async def queue_processor():
    """Background task to process queued requests"""
    while True:
        try:
            await asyncio.sleep(QUEUE_PROCESS_INTERVAL)
            
            if use_in_memory_queue:
                while len(in_memory_webhook_queue) > 0:
                    req = in_memory_webhook_queue.popleft()
                    
                    try:
                        result = await process_solar_rag_request(req.request_body)
                        logger.info(f"âœ… Processed queued request {req.request_id[:8]}")
                    except Exception as e:
                        req.retry_count += 1
                        req.error_message = str(e)
                        
                        if req.retry_count < MAX_RETRY_ATTEMPTS:
                            in_memory_webhook_queue.append(req)
                            logger.warning(f"âš ï¸ Retry {req.retry_count}/{MAX_RETRY_ATTEMPTS} for {req.request_id[:8]}")
                        else:
                            in_memory_failed_queue.append(req)
                            logger.error(f"âŒ Request {req.request_id[:8]} moved to failed queue")
                continue
            
            if not redis_client:
                continue
            
            # Process from Redis queue
            req_json = await redis_client.rpoplpush(WEBHOOK_QUEUE_NAME, WEBHOOK_PROCESSING_QUEUE)
            
            if not req_json:
                continue
            
            req = QueuedRequest.model_validate_json(req_json)
            
            try:
                result = await process_solar_rag_request(req.request_body)
                await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, req_json)
                logger.info(f"âœ… Processed queued request {req.request_id[:8]}")
                
            except Exception as e:
                req.retry_count += 1
                req.error_message = str(e)
                
                await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, req_json)
                
                if req.retry_count < MAX_RETRY_ATTEMPTS:
                    await redis_client.lpush(WEBHOOK_QUEUE_NAME, req.model_dump_json())
                    logger.warning(f"âš ï¸ Retry {req.retry_count}/{MAX_RETRY_ATTEMPTS} for {req.request_id[:8]}")
                else:
                    await redis_client.lpush(WEBHOOK_FAILED_QUEUE, req.model_dump_json())
                    logger.error(f"âŒ Request {req.request_id[:8]} moved to failed queue")
                    
        except Exception as e:
            logger.error(f"âŒ Queue processor error: {e}")

# ================================================================================
# Core Processing Functions
# ================================================================================

async def process_solar_rag_request(request_body: dict) -> dict:
    """Process request with Solar API + RAG or News context"""
    try:
        action = request_body.get("action", {})
        detail_params = action.get("detailParams", {})
        user_message_data = detail_params.get("prompt", {})
        user_message = user_message_data.get("value", "").strip()
        
        # ì‚¬ìš©ì ID ì¶”ì¶œ (ì¹´ì¹´ì˜¤í†¡ userRequestì—ì„œ)
        user_request = request_body.get("userRequest", {})
        user_info = user_request.get("user", {})
        user_id = user_info.get("id", "default")
        
        if not user_message:
            return {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {"simpleText": {"text": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."}}
                    ]
                }
            }
        
        logger.info(f"ğŸ’¬ User message: {user_message}")
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°ì •: ë‰´ìŠ¤ ì„¸ì…˜ì´ ìˆìœ¼ë©´ ë‰´ìŠ¤, ì—†ìœ¼ë©´ RAG
        context = ""
        context_source = "general"
        
        # ë‰´ìŠ¤ ì„¸ì…˜ í™•ì¸
        if user_id in news_sessions:
            news_data = news_sessions[user_id]
            context = f"ë‹¤ìŒì€ ìµœì‹  ë¶€ë™ì‚° ë‰´ìŠ¤ì…ë‹ˆë‹¤:\n\nì œëª©: {news_data['title']}\n\n{news_data['content']}\n\nìœ„ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."
            context_source = "news"
            logger.info(f"ğŸ“° Using news context for user {user_id}")
        else:
            # RAG ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
            context = await get_relevant_context(user_message, top_n=2)
            if context:
                context = f"ë‹¤ìŒì€ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n{context}\n\nìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."
                context_source = "rag"
                logger.info(f"ğŸ“š Using RAG context")
        
        # System prompt êµ¬ì„±
        system_prompt = "ë‹¹ì‹ ì€ ë¶€ë™ì‚° ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ REXAì…ë‹ˆë‹¤."
        if context:
            system_prompt += f"\n\n{context}"
        
        # Solar API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="solar-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            timeout=API_TIMEOUT
        )
        
        ai_response = response.choices[0].message.content.strip()
        
        logger.info(f"âœ… Solar API response received (context: {context_source})")
        logger.info(f"ğŸ“ Response: {ai_response[:100]}...")
        
        # ë‰´ìŠ¤ ëª¨ë“œì¼ ê²½ìš° Quick Reply ì¶”ê°€
        kakao_response = {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": ai_response}}
                ]
            }
        }
        
        if context_source == "news":
            news_data = news_sessions[user_id]
            kakao_response["template"]["quickReplies"] = [
                {
                    "label": "ë‰´ìŠ¤ ì›ë¬¸ ë³´ê¸°",
                    "action": "webLink",
                    "webLinkUrl": news_data['url']
                },
                {
                    "label": "ë‹¤ë¥¸ ì§ˆë¬¸í•˜ê¸°",
                    "action": "message",
                    "messageText": "ì´ ë‰´ìŠ¤ì—ì„œ í•µì‹¬ì€ ë­ì•¼?"
                }
            ]
        
        return kakao_response
        
    except Exception as e:
        logger.error(f"âŒ Solar API error: {type(e).__name__}: {e}")
        raise

# ================================================================================
# API Endpoints
# ================================================================================

@app.post("/generate")
async def generate(request: RequestBody):
    """REXA ë¶€ë™ì‚° ì „ë¬¸ ì±—ë´‡ with RAG - ì¹´ì¹´ì˜¤í†¡ 5ì´ˆ ì œí•œ ëŒ€ì‘"""
    request_id = str(uuid.uuid4())
    
    logger.info("="*50)
    logger.info(f"ğŸ“¨ New RAG request received: {request_id[:8]}")
    logger.info(f"ğŸ“‹ Full request body: {request.model_dump()}")
    
    try:
        # 3ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ ì‹œë„
        result = await process_solar_rag_request(request.model_dump())
        logger.info(f"âœ… Request {request_id[:8]} completed successfully")
        return result
        
    except APITimeoutError as e:
        logger.warning(f"â° Timeout (3s) - enqueueing request {request_id}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ë‹µë³€ ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        }
                    }
                ]
            }
        }
        
    except OpenAIError as e:
        logger.error(f"âŒ API Error: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        }
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error: {type(e).__name__}: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
                        }
                    }
                ]
            }
        }

@app.post("/news")
async def news_bot(request: RequestBody):
    """ë¶€ë™ì‚° ë‰´ìŠ¤ë´‡ - ì¦‰ì‹œ ì‘ë‹µ í›„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì €ì¥"""
    request_id = str(uuid.uuid4())
    
    logger.info("="*50)
    logger.info(f"ğŸ“° News bot request received: {request_id[:8]}")
    
    try:
        # ì‚¬ìš©ì ID ì¶”ì¶œ
        request_dict = request.model_dump()
        user_request = request_dict.get("userRequest", {})
        user_info = user_request.get("user", {})
        user_id = user_info.get("id", "default")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (5ê°œ)
        news_items = search_naver_news("ë¶€ë™ì‚°", display=5)
        
        if not news_items or len(news_items) == 0:
            return {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {"simpleText": {"text": "ë‰´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}}
                    ]
                }
            }
        
        logger.info(f"ğŸ“Š ì´ {len(news_items)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
        
        # ì²« ë²ˆì§¸ ë‰´ìŠ¤ë§Œ ì¦‰ì‹œ í¬ë¡¤ë§ (ì‚¬ìš©ì ì‘ë‹µìš©)
        first_news = news_items[0]
        first_news_content = crawl_news_content(first_news['link'])
        
        # ì„¸ì…˜ì— ì €ì¥ (ì§ˆì˜ì‘ë‹µìš©)
        news_sessions[user_id] = {
            "title": first_news['title'],
            "description": first_news['description'],
            "content": first_news_content,
            "url": first_news['link'],
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… ì²« ë²ˆì§¸ ë‰´ìŠ¤: {first_news['title'][:50]}...")
        
        # Solar AI ìš”ì•½ ìƒì„±
        try:
            summary_prompt = f"ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ 3-4ê°œì˜ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ë¬¸ì¥ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.\n\nì œëª©: {first_news['title']}\n\në³¸ë¬¸: {first_news_content[:1500]}"
            
            response = client.chat.completions.create(
                model="solar-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë‰´ìŠ¤ ì „ë¬¸ ìš”ì•½ AIì…ë‹ˆë‹¤. í•­ìƒ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": summary_prompt}
                ],
                max_tokens=300,
                timeout=API_TIMEOUT
            )
            
            summary = response.choices[0].message.content.strip()
            logger.info(f"âœ… Solar AI summary generated")
            
        except Exception as e:
            logger.error(f"âŒ Summary generation failed: {e}")
            summary = first_news['description']
            last_period = summary.rfind('.')
            if last_period > 0:
                summary = summary[:last_period + 1]
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…: ëª¨ë“  ë‰´ìŠ¤ ì €ì¥ (ë¹„ë™ê¸°)
        asyncio.create_task(save_all_news_background(news_items, user_id))
        
        # ì‚¬ìš©ìì—ê²Œ ì¦‰ì‹œ ì‘ë‹µ
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": f"ğŸ“° {first_news['title']}\n\n{summary}\n\nğŸ”— {first_news['link']}\n\nğŸ’¬ ì´ ë‰´ìŠ¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"
                        }
                    }
                ],
                "quickReplies": [
                    {
                        "label": "í•µì‹¬ ë‚´ìš©ì€?",
                        "action": "message",
                        "messageText": "ì´ ë‰´ìŠ¤ì˜ í•µì‹¬ì€ ë­ì•¼?"
                    },
                    {
                        "label": "ì‹œì¥ ì˜í–¥ì€?",
                        "action": "message",
                        "messageText": "ì´ê²Œ ë¶€ë™ì‚° ì‹œì¥ì— ì–´ë–¤ ì˜í–¥ì„ ì¤„ê¹Œ?"
                    },
                    {
                        "label": "ì›ë¬¸ ë³´ê¸°",
                        "action": "webLink",
                        "webLinkUrl": first_news['link']
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ News bot error: {type(e).__name__}: {e}")
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": "ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}}
                ]
            }
        }

async def save_all_news_background(news_items: list, user_id: str):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë“  ë‰´ìŠ¤ ì €ì¥ (í•„í„°ë§ ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    logger.info(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ì €ì¥ ì‹œì‘: {len(news_items)}ê°œ")
    saved_count = 0
    
    for idx, news_item in enumerate(news_items):
        try:
            # Rate Limit ë°©ì§€
            if idx > 0:
                await asyncio.sleep(2)
            
            # ğŸ†• í‚¤ ì´ë¦„ í†µì¼ (link â†’ url)
            if 'link' in news_item and 'url' not in news_item:
                news_item['url'] = news_item['link']
            
            # user_id ì¶”ê°€
            news_item['user_id'] = user_id
            
            # í•„í„°ë§ ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            if 'is_relevant' not in news_item:
                news_item['is_relevant'] = True
                news_item['relevance_score'] = 50
                news_item['keywords'] = []
                news_item['region'] = ''
                news_item['has_price'] = False
                news_item['has_policy'] = False
                news_item['reason'] = 'Filtering module not available'
            
            # ì €ì¥ (í•„í„°ë§ ì •ë³´ í¬í•¨)
            save_news_to_csv(news_item)
            save_news_to_gsheet(news_item)
            
            saved_count += 1
            logger.info(
                f"âœ… [{saved_count}/{len(news_items)}] ì €ì¥ ì™„ë£Œ "
                f"[{news_item.get('relevance_score', 0)}ì ] "
                f"{news_item['title'][:30]}..."
            )
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ {idx+1} ì €ì¥ ì‹¤íŒ¨: {e}")
            logger.error(f"   news_item keys: {news_item.keys()}")
            continue
    
    logger.info(f"ğŸ‰ ë°±ê·¸ë¼ìš´ë“œ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")

@app.post("/custom")
async def generate_custom(request: RequestBody):
    """REXA ë¶€ë™ì‚° ì „ë¬¸ ì±—ë´‡ with RAG - ì¹´ì¹´ì˜¤í†¡ 5ì´ˆ ì œí•œ ëŒ€ì‘"""
    request_id = str(uuid.uuid4())
    
    logger.info("="*50)
    logger.info(f"ğŸ“¨ New RAG request received: {request_id[:8]}")
    logger.info(f"ğŸ“‹ Full request body: {request.model_dump()}")
    
    try:
        # 3ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë¹ ë¥¸ ì‘ë‹µ ì‹œë„
        result = await process_solar_rag_request(request.model_dump())
        logger.info(f"âœ… Request {request_id[:8]} completed successfully")
        return result
        
    except APITimeoutError as e:
        logger.warning(f"â° Timeout (3s) - enqueueing request {request_id}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ë‹µë³€ ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                        }
                    }
                ]
            }
        }
        
    except OpenAIError as e:
        logger.error(f"âŒ API Error: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        }
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error: {type(e).__name__}: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œë²ˆ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
                        }
                    }
                ]
            }
        }

@app.get("/health")
async def health_check() -> HealthStatus:
    """Enhanced health check endpoint"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return HealthStatus(
        status="healthy" if server_healthy else "unhealthy",
        model="solar-mini",
        mode="rexa_chatbot_rag_news",
        server_healthy=server_healthy,
        last_check=last_health_check.isoformat(),
        redis_connected=(redis_client is not None and not use_in_memory_queue),
        queue_size=queue_size,
        processing_queue_size=processing_size,
        failed_queue_size=failed_size
    )

@app.get("/health/ping")
async def health_ping():
    """Simple ping endpoint for client health checks"""
    return {
        "alive": True,
        "healthy": server_healthy,
        "timestamp": datetime.now().isoformat(),
        "rag_enabled": len(chunk_embeddings) > 0,
        "news_sessions": len(news_sessions)
    }

@app.get("/debug/env")
async def debug_env():
    """í™˜ê²½ ë³€ìˆ˜ ì²´í¬ (ë””ë²„ê¹…ìš©)"""
    return {
        "google_sheets_credentials_exists": bool(GOOGLE_SHEETS_CREDENTIALS),
        "google_sheets_credentials_length": len(GOOGLE_SHEETS_CREDENTIALS) if GOOGLE_SHEETS_CREDENTIALS else 0,
        "google_sheets_spreadsheet_id_exists": bool(GOOGLE_SHEETS_SPREADSHEET_ID),
        "google_sheets_spreadsheet_id": GOOGLE_SHEETS_SPREADSHEET_ID if GOOGLE_SHEETS_SPREADSHEET_ID else "NOT_SET",
        "gspread_available": GSPREAD_AVAILABLE,
        "gsheet_client_initialized": gsheet_client is not None,
        "gsheet_worksheet_initialized": gsheet_worksheet is not None,
        "naver_client_id_exists": bool(NAVER_CLIENT_ID),
        "naver_client_secret_exists": bool(NAVER_CLIENT_SECRET)
    }

@app.get("/queue/status")
async def queue_status():
    """Get detailed queue status"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return {
        "queue_type": "in-memory" if use_in_memory_queue else "redis",
        "webhook_queue": queue_size,
        "processing_queue": processing_size,
        "failed_queue": failed_size,
        "total": queue_size + processing_size + failed_size,
        "rag_chunks_loaded": len(article_chunks),
        "active_news_sessions": len(news_sessions)
    }

@app.post("/queue/retry-failed")
async def retry_failed_requests():
    """Manually retry all failed requests"""
    try:
        if use_in_memory_queue:
            retry_count = len(in_memory_failed_queue)
            while len(in_memory_failed_queue) > 0:
                req = in_memory_failed_queue.pop()
                req.retry_count = 0
                in_memory_webhook_queue.appendleft(req)
            
            logger.info(f"âœ… Retrying {retry_count} failed requests (in-memory)")
            return {"retried": retry_count, "queue_type": "in-memory"}
        
        if not redis_client:
            return {"error": "Queue not available"}
        
        failed_items = await redis_client.lrange(WEBHOOK_FAILED_QUEUE, 0, -1)
        retry_count = 0
        
        for item in failed_items:
            req = QueuedRequest.model_validate_json(item)
            req.retry_count = 0
            await redis_client.lpush(WEBHOOK_QUEUE_NAME, req.model_dump_json())
            retry_count += 1
        
        await redis_client.delete(WEBHOOK_FAILED_QUEUE)
        
        logger.info(f"âœ… Retrying {retry_count} failed requests (Redis)")
        return {"retried": retry_count, "queue_type": "redis"}
        
    except Exception as e:
        logger.error(f"âŒ Failed to retry requests: {e}")
        return {"error": str(e)}

# ================================================================================
# Startup & Shutdown Events
# ================================================================================

@app.on_event("startup")
async def startup_event():
    """Initialize resources on startup"""
    logger.info("="*70)
    logger.info("ğŸš€ Starting REXA server (Solar + RAG + News + Filtering)...")
    logger.info("="*70)
    
    # RAG ìƒíƒœ í™•ì¸
    if len(chunk_embeddings) > 0:
        logger.info(f"âœ… RAG ENABLED: {len(chunk_embeddings)} chunks loaded")
    else:
        logger.warning("âš ï¸ RAG DISABLED: No embeddings loaded")
        logger.warning("âš ï¸ Server will work but without company-specific knowledge")
    
    # Naver API í™•ì¸
    if NAVER_CLIENT_ID and NAVER_CLIENT_SECRET:
        logger.info("âœ… Naver News API configured")
    else:
        logger.warning("âš ï¸ Naver News API not configured")
    
    # ğŸ†• News Filtering í™•ì¸
    if NEWS_FILTER_AVAILABLE:
        logger.info("âœ… News filtering system enabled")
    else:
        logger.warning("âš ï¸ News filtering system disabled")
        logger.warning("   Place news_filter_simple.py in the same directory")
    
    # CSV ì´ˆê¸°í™”
    csv_success = init_csv_file()
    if csv_success:
        logger.info("âœ… CSV logging enabled (with filtering columns)")
    else:
        logger.warning("âš ï¸ CSV logging disabled")
    
    # Google Sheets ì´ˆê¸°í™”
    gsheet_success = init_google_sheets()
    if gsheet_success:
        logger.info("âœ… Google Sheets logging enabled (with filtering columns)")
    else:
        logger.warning("âš ï¸ Google Sheets logging disabled")
    
    # Redis ì´ˆê¸°í™”
    await init_redis()
    
    # Background tasks
    asyncio.create_task(health_check_monitor())
    asyncio.create_task(queue_processor())
    
    logger.info("="*70)
    logger.info("âœ… REXA server startup complete!")
    logger.info(f"   - Model: solar-mini")
    logger.info(f"   - RAG chunks: {len(chunk_embeddings)}")
    logger.info(f"   - Redis: {'connected' if redis_client else 'in-memory queue'}")
    logger.info(f"   - News API: {'enabled' if NAVER_CLIENT_ID else 'disabled'}")
    logger.info(f"   - News Filter: {'enabled' if NEWS_FILTER_AVAILABLE else 'disabled'}")
    logger.info(f"   - CSV logging: {'enabled' if csv_success else 'disabled'}")
    logger.info(f"   - Google Sheets: {'enabled' if gsheet_success else 'disabled'}")
    logger.info("="*70)

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup resources on shutdown"""
    logger.info("ğŸ‘‹ Shutting down REXA server (Solar + RAG + News)...")
    await close_redis()
    logger.info("âœ… REXA server shut down successfully")
