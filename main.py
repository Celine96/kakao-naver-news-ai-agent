"""
REXA Ïπ¥Ïπ¥Ïò§ÌÜ° Î¥á ÏÑúÎ≤Ñ
- ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Ïãú Îâ¥Ïä§ Ï†ÑÎã¨
- RAG Í∏∞Î∞ò ÏßàÏùòÏùëÎãµ
"""

import logging
import os
import asyncio
import uuid
from datetime import datetime
from typing import Optional, Any
from collections import deque

from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI, OpenAIError, APITimeoutError
import numpy as np
import pickle

# Í≥µÌÜµ Ìï®Ïàò ÏûÑÌè¨Ìä∏
from common import (
    get_latest_news_from_gsheet,
    save_all_news_background,
    init_google_sheets,
    init_csv_file
)

# Redis for queue management (optional)
try:
    import redis.asyncio as redis
    from redis.asyncio import Redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    Redis = Any
    logging.warning("redis package not installed. Using in-memory queue.")

# ================================================================================
# Î°úÍπÖ ÏÑ§Ï†ï
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="REXA - Real Estate Expert Assistant",
    description="Ïπ¥Ïπ¥Ïò§ÌÜ° Î∂ÄÎèôÏÇ∞ Îâ¥Ïä§Î¥á + RAG",
    version="3.0.0"
)

# ================================================================================
# ÌôòÍ≤ΩÎ≥ÄÏàò & ÏÑ§Ï†ï
# ================================================================================

# Upstage Solar API
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
API_TIMEOUT = int(os.getenv("API_TIMEOUT", 3))

# Redis (optional)
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

# Queue
WEBHOOK_QUEUE_NAME = "rexa:webhook_queue"
WEBHOOK_PROCESSING_QUEUE = "rexa:processing_queue"
WEBHOOK_FAILED_QUEUE = "rexa:failed_queue"
MAX_RETRY_ATTEMPTS = int(os.getenv("MAX_RETRY_ATTEMPTS", 3))
QUEUE_PROCESS_INTERVAL = int(os.getenv("QUEUE_PROCESS_INTERVAL", 5))

# Health Check
HEALTH_CHECK_INTERVAL = int(os.getenv("HEALTH_CHECK_INTERVAL", 5))
MAX_UNHEALTHY_COUNT = int(os.getenv("MAX_UNHEALTHY_COUNT", 3))

# ================================================================================
# Í∏ÄÎ°úÎ≤å Î≥ÄÏàò
# ================================================================================

# Solar API client
client = OpenAI(
    api_key=UPSTAGE_API_KEY,
    base_url="https://api.upstage.ai/v1/solar",
    timeout=API_TIMEOUT
)

# Redis
redis_client: Optional[Any] = None
use_in_memory_queue = False

# In-memory queue fallback
in_memory_webhook_queue: deque = deque()
in_memory_processing_queue: deque = deque()
in_memory_failed_queue: deque = deque()

# Health
server_healthy = True
unhealthy_count = 0
last_health_check = datetime.now()

# News sessions (user_id -> news_data)
news_sessions = {}

# RAG embeddings
article_chunks = []
chunk_embeddings = []

# ================================================================================
# Pydantic Î™®Îç∏
# ================================================================================

class DetailParams(BaseModel):
    prompt: dict

class Action(BaseModel):
    params: dict
    detailParams: dict

class RequestBody(BaseModel):
    action: Action

class QueuedRequest(BaseModel):
    request_id: str
    request_body: dict
    timestamp: str
    retry_count: int = 0
    error_message: Optional[str] = None

class HealthStatus(BaseModel):
    status: str
    model: str
    mode: str
    server_healthy: bool
    last_check: str
    redis_connected: bool
    queue_size: int
    processing_queue_size: int
    failed_queue_size: int

# ================================================================================
# RAG ÏûÑÎ≤†Îî© Î°úÎìú
# ================================================================================

try:
    with open("embeddings.pkl", "rb") as f:
        data = pickle.load(f)
        article_chunks = data["chunks"]
        chunk_embeddings = data["embeddings"]
    logger.info(f"‚úÖ Loaded {len(article_chunks)} chunks from embeddings.pkl")
except FileNotFoundError:
    logger.warning("‚ö†Ô∏è embeddings.pkl not found - RAG disabled")
except Exception as e:
    logger.error(f"‚ùå Failed to load embeddings: {e}")

# ================================================================================
# RAG Helper Functions
# ================================================================================

def cosine_similarity(a, b):
    """Calculate cosine similarity"""
    from numpy import dot
    from numpy.linalg import norm
    return dot(a, b) / (norm(a) * norm(b))

async def get_relevant_context(prompt: str, top_n: int = 2) -> str:
    """Get relevant context from embeddings for RAG"""
    if not chunk_embeddings or not article_chunks:
        return ""
    
    try:
        embedding_dim = len(chunk_embeddings[0])
        
        # Solar embedding
        q_embedding = client.embeddings.create(
            input=prompt,
            model="solar-embedding-1-large-query"
        ).data[0].embedding
        
        # Calculate similarities
        similarities = [cosine_similarity(q_embedding, emb) for emb in chunk_embeddings]
        top_indices = np.argsort(similarities)[-top_n:][::-1]
        selected_context = "\n\n".join([article_chunks[i] for i in top_indices])
        
        return selected_context
    except Exception as e:
        logger.error(f"‚ùå Error getting relevant context: {e}")
        return ""

# ================================================================================
# Redis & Queue Í¥ÄÎ¶¨
# ================================================================================

async def init_redis():
    """Initialize Redis connection"""
    global redis_client, use_in_memory_queue
    
    if not REDIS_AVAILABLE:
        logger.warning("‚ö†Ô∏è Redis package not installed - using in-memory queue")
        use_in_memory_queue = True
        return
    
    try:
        redis_client = await redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            password=REDIS_PASSWORD,
            decode_responses=True,
            socket_connect_timeout=5
        )
        await redis_client.ping()
        logger.info(f"‚úÖ Redis connected: {REDIS_HOST}:{REDIS_PORT}")
        use_in_memory_queue = False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Redis connection failed: {e}")
        use_in_memory_queue = True

async def close_redis():
    """Close Redis connection"""
    global redis_client
    if redis_client:
        await redis_client.close()
        logger.info("‚úÖ Redis connection closed")

async def enqueue_webhook_request(request_id: str, request_body: dict):
    """Add webhook request to queue"""
    queued_req = QueuedRequest(
        request_id=request_id,
        request_body=request_body,
        timestamp=datetime.now().isoformat()
    )
    
    if use_in_memory_queue:
        in_memory_webhook_queue.append(queued_req)
        logger.info(f"üì• Request queued (in-memory): {len(in_memory_webhook_queue)}")
        return
    
    if not redis_client:
        in_memory_webhook_queue.append(queued_req)
        return
    
    try:
        await redis_client.lpush(WEBHOOK_QUEUE_NAME, queued_req.model_dump_json())
        queue_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        logger.info(f"üì• Request queued (Redis): {queue_size}")
    except Exception as e:
        logger.error(f"‚ùå Failed to enqueue: {e}")
        in_memory_webhook_queue.append(queued_req)

async def get_queue_sizes():
    """Get sizes of all queues"""
    if use_in_memory_queue:
        return (
            len(in_memory_webhook_queue),
            len(in_memory_processing_queue),
            len(in_memory_failed_queue)
        )
    
    if not redis_client:
        return (0, 0, 0)
    
    try:
        webhook_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        processing_size = await redis_client.llen(WEBHOOK_PROCESSING_QUEUE)
        failed_size = await redis_client.llen(WEBHOOK_FAILED_QUEUE)
        return (webhook_size, processing_size, failed_size)
    except:
        return (0, 0, 0)

# ================================================================================
# Background Tasks
# ================================================================================

async def health_check_monitor():
    """Background task to monitor server health"""
    global server_healthy, unhealthy_count, last_health_check
    
    while True:
        try:
            await asyncio.sleep(HEALTH_CHECK_INTERVAL)
            
            # Check Solar API
            try:
                test_response = client.chat.completions.create(
                    model="solar-mini",
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=10,
                    timeout=2
                )
                
                server_healthy = True
                unhealthy_count = 0
                last_health_check = datetime.now()
                
            except Exception as e:
                unhealthy_count += 1
                logger.warning(f"‚ö†Ô∏è Health check failed ({unhealthy_count}/{MAX_UNHEALTHY_COUNT})")
                
                if unhealthy_count >= MAX_UNHEALTHY_COUNT:
                    server_healthy = False
                    logger.error(f"‚ùå Server marked as unhealthy")
                
        except Exception as e:
            logger.error(f"‚ùå Health check monitor error: {e}")

async def queue_processor():
    """Background task to process queued requests"""
    while True:
        try:
            await asyncio.sleep(QUEUE_PROCESS_INTERVAL)
            
            if use_in_memory_queue:
                while len(in_memory_webhook_queue) > 0:
                    req = in_memory_webhook_queue.popleft()
                    
                    try:
                        result = await process_solar_rag_request(req.request_body)
                        logger.info(f"‚úÖ Processed queued request {req.request_id[:8]}")
                    except Exception as e:
                        req.retry_count += 1
                        req.error_message = str(e)
                        
                        if req.retry_count < MAX_RETRY_ATTEMPTS:
                            in_memory_webhook_queue.append(req)
                        else:
                            in_memory_failed_queue.append(req)
                            logger.error(f"‚ùå Request {req.request_id[:8]} failed")
                continue
            
            # Redis queue processing (ÏÉùÎûµ Í∞ÄÎä•)
            
        except Exception as e:
            logger.error(f"‚ùå Queue processor error: {e}")

# ================================================================================
# Core Processing Functions
# ================================================================================

async def process_solar_rag_request(request_body: dict) -> dict:
    """Process request with Solar API + RAG or News context"""
    try:
        action = request_body.get("action", {})
        detail_params = action.get("detailParams", {})
        user_message_data = detail_params.get("prompt", {})
        user_message = user_message_data.get("value", "").strip()
        
        # ÏÇ¨Ïö©Ïûê ID Ï∂îÏ∂ú
        user_request = request_body.get("userRequest", {})
        user_info = user_request.get("user", {})
        user_id = user_info.get("id", "default")
        
        if not user_message:
            return {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {"simpleText": {"text": "ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî."}}
                    ]
                }
            }
        
        logger.info(f"üí¨ User message: {user_message}")
        
        # Ïª®ÌÖçÏä§Ìä∏ Í≤∞Ï†ï
        context = ""
        context_source = "general"
        
        # Îâ¥Ïä§ ÏÑ∏ÏÖò ÌôïÏù∏
        if user_id in news_sessions:
            news_data = news_sessions[user_id]
            context = f"Îã§ÏùåÏùÄ ÏµúÏã† Î∂ÄÎèôÏÇ∞ Îâ¥Ïä§ÏûÖÎãàÎã§:\n\nÏ†úÎ™©: {news_data['title']}\n\n{news_data['content']}\n\nÏúÑ Îâ¥Ïä§Î•º Ï∞∏Í≥†ÌïòÏó¨ ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî."
            context_source = "news"
            logger.info(f"üì∞ Using news context")
        else:
            # RAG Ïª®ÌÖçÏä§Ìä∏ ÏÇ¨Ïö©
            context = await get_relevant_context(user_message, top_n=2)
            if context:
                context = f"Îã§ÏùåÏùÄ Í¥ÄÎ†® Ï†ïÎ≥¥ÏûÖÎãàÎã§:\n\n{context}\n\nÏúÑ Ï†ïÎ≥¥Î•º Ï∞∏Í≥†ÌïòÏó¨ ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî."
                context_source = "rag"
                logger.info(f"üìö Using RAG context")
        
        # System prompt
        system_prompt = "ÎãπÏã†ÏùÄ Î∂ÄÎèôÏÇ∞ Ï†ÑÎ¨∏ AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ REXAÏûÖÎãàÎã§."
        if context:
            system_prompt += f"\n\n{context}"
        
        # Solar API Ìò∏Ï∂ú
        response = client.chat.completions.create(
            model="solar-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            max_tokens=500,
            timeout=API_TIMEOUT
        )
        
        ai_response = response.choices[0].message.content.strip()
        logger.info(f"‚úÖ Solar API response (context: {context_source})")
        
        # Ïπ¥Ïπ¥Ïò§ÌÜ° ÏùëÎãµ
        kakao_response = {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": ai_response}}
                ]
            }
        }
        
        # Îâ¥Ïä§ Î™®ÎìúÏùº Í≤ΩÏö∞ Quick Reply Ï∂îÍ∞Ä
        if context_source == "news":
            news_data = news_sessions[user_id]
            kakao_response["template"]["quickReplies"] = [
                {
                    "label": "Îâ¥Ïä§ ÏõêÎ¨∏ Î≥¥Í∏∞",
                    "action": "webLink",
                    "webLinkUrl": news_data['url']
                },
                {
                    "label": "Îã§Î•∏ ÏßàÎ¨∏ÌïòÍ∏∞",
                    "action": "message",
                    "messageText": "Ïù¥ Îâ¥Ïä§ÏóêÏÑú ÌïµÏã¨ÏùÄ Î≠êÏïº?"
                }
            ]
        
        return kakao_response
        
    except Exception as e:
        logger.error(f"‚ùå Solar API error: {e}")
        raise

# ================================================================================
# API ÏóîÎìúÌè¨Ïù∏Ìä∏
# ================================================================================

@app.post("/news")
async def news_bot(request: RequestBody):
    """Î∂ÄÎèôÏÇ∞ Îâ¥Ïä§Î¥á - Í∞ÑÍ≤∞Ìïú Î¶¨Ïä§Ìä∏ ÌòïÏãù (5Í∞ú)"""
    request_id = str(uuid.uuid4())
    
    logger.info("=" * 50)
    logger.info(f"üì∞ News bot request: {request_id[:8]}")
    
    try:
        # ÏÇ¨Ïö©Ïûê ID Ï∂îÏ∂ú
        request_dict = request.model_dump()
        user_request = request_dict.get("userRequest", {})
        user_info = user_request.get("user", {})
        user_id = user_info.get("id", "default")
        
        # Íµ¨Í∏Ä ÏãúÌä∏ÏóêÏÑú ÏµúÏã† Îâ¥Ïä§ 5Í∞ú Ï°∞Ìöå (0.1Ï¥à)
        news_items = get_latest_news_from_gsheet(limit=5)
        
        if not news_items or len(news_items) == 0:
            logger.warning("‚ö†Ô∏è Íµ¨Í∏Ä ÏãúÌä∏Ïóê Îâ¥Ïä§ ÏóÜÏùå")
            return {
                "version": "2.0",
                "template": {
                    "outputs": [
                        {"simpleText": {"text": "ÏµúÏã† Îâ¥Ïä§Î•º Ï§ÄÎπÑ Ï§ëÏûÖÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."}}
                    ]
                }
            }
        
        logger.info(f"‚úÖ Íµ¨Í∏Ä ÏãúÌä∏ Ï°∞Ìöå ÏôÑÎ£å: {len(news_items)}Í∞ú")
        
        # Î°úÍπÖ
        for idx, item in enumerate(news_items, 1):
            logger.info(
                f"   [{idx}] {item['title'][:40]}... "
                f"(Ï†êÏàò: {item.get('relevance_score', 0)})"
            )
        
        # Îâ¥Ïä§ Î¶¨Ïä§Ìä∏ ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
        news_list = f"üì∞ Ïò§ÎäòÏùò Î∂ÄÎèôÏÇ∞ Îâ¥Ïä§ (Ï¥ù {len(news_items)}Í±¥)\n\n"
        
        for idx, item in enumerate(news_items, 1):
            title = item.get('title', 'Ï†úÎ™© ÏóÜÏùå')
            url = item.get('link', '')
            
            # ÎîîÎ≤ÑÍπÖ: URL ÌôïÏù∏
            logger.info(f"   Îâ¥Ïä§ {idx}: URL = {url[:50] if url else 'URL ÏóÜÏùå!'}")
            
            # URLÏù¥ ÏóÜÏúºÎ©¥ Í≤ΩÍ≥†
            if not url:
                logger.warning(f"   ‚ö†Ô∏è Îâ¥Ïä§ {idx} URL ÏóÜÏùå: {title[:30]}")
                url = "(URL Ï†ïÎ≥¥ ÏóÜÏùå)"
            
            # Ï†úÎ™© + URL (URLÏùÑ Î≥ÑÎèÑ Ï§ÑÏóê ÌëúÏãú)
            news_list += f"{idx}. {title}\n{url}\n\n"
        
        # Ï≤´ Î≤àÏß∏ Îâ¥Ïä§ ÏÑ∏ÏÖòÏóê Ï†ÄÏû• (ÎåÄÌôî Ïù¥Ïñ¥Í∞ÄÍ∏∞Ïö©)
        first_news = news_items[0]
        news_sessions[user_id] = {
            "title": first_news['title'],
            "description": first_news['description'],
            "content": first_news.get('summary', first_news['description']),
            "url": first_news['link'],
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"‚úÖ Ï¥àÍ≥†ÏÜç ÏùëÎãµ ÏôÑÎ£å (0.1Ï¥à)")
        
        # Ïπ¥Ïπ¥Ïò§ÌÜ° ÏùëÎãµ - simpleText (URL Ìè¨Ìï®)
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": news_list.strip()
                        }
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå News bot error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": "Îâ¥Ïä§Î•º Î∂àÎü¨Ïò§Îäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."}}
                ]
            }
        }

@app.post("/custom")
async def generate_custom(request: RequestBody):
    """REXA Ï±óÎ¥á (RAG) - Ïπ¥Ïπ¥Ïò§ÌÜ° 5Ï¥à Ï†úÌïú ÎåÄÏùë"""
    request_id = str(uuid.uuid4())
    
    logger.info("=" * 50)
    logger.info(f"üì® RAG request: {request_id[:8]}")
    
    try:
        result = await process_solar_rag_request(request.model_dump())
        logger.info(f"‚úÖ Request {request_id[:8]} completed")
        return result
        
    except APITimeoutError:
        logger.warning(f"‚è∞ Timeout - enqueueing {request_id}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": "ÎãµÎ≥Ä ÏÉùÏÑ±Ïóê ÏãúÍ∞ÑÏù¥ Í±∏Î¶¨Í≥† ÏûàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏßàÎ¨∏Ìï¥Ï£ºÏÑ∏Ïöî."}}
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {"simpleText": {"text": "Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÌïúÎ≤à ÏßàÎ¨∏Ìï¥Ï£ºÏÑ∏Ïöî."}}
                ]
            }
        }

@app.get("/health")
async def health_check() -> HealthStatus:
    """Health check endpoint"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return HealthStatus(
        status="healthy" if server_healthy else "unhealthy",
        model="solar-mini",
        mode="rexa_chatbot_rag_news",
        server_healthy=server_healthy,
        last_check=last_health_check.isoformat(),
        redis_connected=(redis_client is not None and not use_in_memory_queue),
        queue_size=queue_size,
        processing_queue_size=processing_size,
        failed_queue_size=failed_size
    )

@app.get("/health/ping")
async def health_ping():
    """Simple ping endpoint"""
    return {
        "alive": True,
        "healthy": server_healthy,
        "timestamp": datetime.now().isoformat(),
        "rag_enabled": len(chunk_embeddings) > 0,
        "news_sessions": len(news_sessions)
    }

# ================================================================================
# Startup & Shutdown
# ================================================================================

@app.on_event("startup")
async def startup_event():
    """Initialize resources on startup"""
    logger.info("=" * 70)
    logger.info("üöÄ Starting REXA Bot Server...")
    logger.info("=" * 70)
    
    # RAG ÏÉÅÌÉú
    if len(chunk_embeddings) > 0:
        logger.info(f"‚úÖ RAG ENABLED: {len(chunk_embeddings)} chunks")
    else:
        logger.warning("‚ö†Ô∏è RAG DISABLED: No embeddings loaded")
    
    # CSV/Sheets Ï¥àÍ∏∞Ìôî
    csv_success = init_csv_file()
    gsheet_success = init_google_sheets()
    
    if csv_success:
        logger.info("‚úÖ CSV logging enabled")
    if gsheet_success:
        logger.info("‚úÖ Google Sheets logging enabled")
    
    # Redis Ï¥àÍ∏∞Ìôî
    await init_redis()
    
    # Background tasks
    asyncio.create_task(health_check_monitor())
    asyncio.create_task(queue_processor())
    
    logger.info("=" * 70)
    logger.info("‚úÖ REXA Bot Server started!")
    logger.info(f"   - Model: solar-mini")
    logger.info(f"   - RAG chunks: {len(chunk_embeddings)}")
    logger.info(f"   - Redis: {'connected' if redis_client else 'in-memory'}")
    logger.info("=" * 70)

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup resources"""
    logger.info("üëã Shutting down REXA Bot Server...")
    await close_redis()
    logger.info("‚úÖ Shutdown complete")
